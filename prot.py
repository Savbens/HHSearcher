import time
import requests
import pdfplumber
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer, util
from collections import defaultdict
import re

def clean_text(text: str) -> str:
    text = re.sub(r'\s+', ' ', text)
    return text.strip('‚Ä¢‚Äî\n\t ')

def extract_resume_sections(pdf_path: str) -> dict[str, str]:
    sections = defaultdict(list)
    current = '–∫–æ–Ω—Ç–∞–∫—Ç—ã'
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            raw = page.extract_text(
                x_tolerance=3,
                y_tolerance=3,
                layout=True,
                keep_blank_chars=False
            )
            if not raw:
                continue
            lines = [clean_text(line) for line in raw.split('\n') if clean_text(line)]
            for line in lines:
                if "–æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã" in line.lower():
                    current = "–æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã"
                elif "–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ" in line.lower():
                    current = "–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ"
                elif "–Ω–∞–≤—ã–∫–∏" in line.lower():
                    current = "–Ω–∞–≤—ã–∫–∏"
                elif "–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è" in line.lower():
                    current = "–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"
                if line and not line.isspace():
                    sections[current].append(line)
    return {sec: "\n".join(blocks).strip() for sec, blocks in sections.items()}

resume_sections = extract_resume_sections('resume1.pdf')
resume_text = ""
if "–æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã" in resume_sections:
    resume_text += "–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã:\n" + resume_sections["–æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã"] + "\n\n"
if "–Ω–∞–≤—ã–∫–∏" in resume_sections:
    resume_text += "–ù–∞–≤—ã–∫–∏ –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏:\n" + resume_sections["–Ω–∞–≤—ã–∫–∏"] + "\n\n"
if "–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ" in resume_sections:
    resume_text += "–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ:\n" + resume_sections["–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ"] + "\n\n"
if "–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è" in resume_sections:
    resume_text += resume_sections["–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"] + "\n\n"
if "–∫–æ–Ω—Ç–∞–∫—Ç—ã" in resume_sections:
    resume_text += resume_sections["–∫–æ–Ω—Ç–∞–∫—Ç—ã"]
print(f"[+] –ò–∑ PDF –∏–∑–≤–ª–µ—á–µ–Ω–æ –∏ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(resume_text)} —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞.")

model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
profile_emb = model.encode(resume_text, convert_to_tensor=True, device='cpu')
print(resume_text)
KEYWORDS = ["machine learning", "data science", "C++ developer"]
LOCATIONS = [
    {"area": 2,     "name": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥"},
    {"remote": True,"name": "–£–¥–∞–ª—ë–Ω–Ω–∞—è"}
]
FILTER_TERMS = ["machine learning", "data science", "–°++"]
session = requests.Session()
session.headers.update({
    'User-Agent':      'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                       'AppleWebKit/537.36 (KHTML, like Gecko) '
                       'Chrome/112.0.0.0 Safari/537.36',
    'Accept-Language': 'ru-RU,ru;q=0.9',
    'Referer':         'https://hh.ru/'
})

def search_vacancies_html(query: str, loc: dict) -> list[dict]:
    url = 'https://hh.ru/search/vacancy'
    params = {'text': query, 'items_on_page': 20}
    if 'area' in loc:
        params['area'] = loc['area']
    if loc.get('remote'):
        params['remote'] = 1
    r = session.get(url, params=params)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, 'html.parser')
    out = {}
    for a in soup.find_all('a', href=True):
        href = a['href']
        if '/vacancy/' not in href:
            continue
        name = a.get_text(strip=True)
        if not name:
            continue
        snippet = ""
        sib = a.find_next_sibling()
        if sib and sib.name == 'div':
            snippet = sib.get_text(" ", strip=True)
        out[href] = {
            'name':     name,
            'url':      href,
            'snippet':  snippet,
            'location': loc['name']
        }
    return list(out.values())

all_vac = []
for loc in LOCATIONS:
    for kw in KEYWORDS:
        items = search_vacancies_html(kw, loc)
        all_vac.extend(items)
        time.sleep(0.5)
unique = {v['url']: v for v in all_vac}.values()
filtered = []
for v in unique:
    text = (v['name'] + " " + v['snippet']).lower()
    if any(term in text for term in FILTER_TERMS):
        filtered.append(v)
print(f"[+] –í—Å–µ–≥–æ –ø–æ—Å–ª–µ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(filtered)} –≤–∞–∫–∞–Ω—Å–∏–π")
if not filtered:
    raise SystemExit("–ù–µ—Ç –Ω–∏ –æ–¥–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Ç–µ—Ä–º–∏–Ω–∞–º.")

def get_full_vacancy_text(url: str) -> str:
    try:
        r = session.get(url, timeout=10)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, 'html.parser')
        desc = soup.find('div', {'data-qa': 'vacancy-description'})
        if desc:
            return desc.get_text(" ", strip=True)
    except Exception as e:
        print(f"[!] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏ {url}: {e}")
    return ""

vac_texts = []
for v in filtered:
    full_text = get_full_vacancy_text(v['url'])
    combined_text = f"{v['name']} {v['snippet']} {full_text}".strip()
    vac_texts.append(combined_text)
    time.sleep(0.5)
vac_embs = model.encode(vac_texts, convert_to_tensor=True, device='cpu')
scores = util.cos_sim(profile_emb, vac_embs)[0]
top5_idx = scores.topk(k=20).indices
print("\nüî• –¢–æ–ø-5 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π:")
for idx in top5_idx:
    v = filtered[idx]
    print(f"- {v['name']} ({v['location']})")
    print(f"    {v['url']}")
    print(f"    –°—Ö–æ–∂–µ—Å—Ç—å: {scores[idx]:.2f}\n")
